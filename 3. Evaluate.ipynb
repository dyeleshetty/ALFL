{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30d8c1c5",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "The evaluation is done after the training, not during the training. This is because the evaluation can then be executed on a seperate computer with less resources, while the powerful computers continue training. Two dataloaders are created, one for the 2-class dataset and one for the 8-class dataset. The validation set is used to calculate the common metrics, which are then stored in a .pickle file, which are used in 4 to analyse and graph the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "comfortable-laser",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import glob\n",
    "import gc\n",
    "import torch\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "radio-injection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Experiments\\ALFL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval2: \u001b[0mScanning \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dataloader = evaluate.get_dataloader(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "associate-calculation",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  5639932 torch 1.8.0+cu111 CUDA:0 (GeForce RTX 3080, 10240.0MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 224 layers, 7056607 parameters, 0 gradients\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36/36 [00:15<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "saving json runs\\val\\fed-12-280\\fed-12-280_predictions.json...\n",
      "                 all        2244       10080       0.797       0.686       0.753       0.403\n",
      "                 car        2244        8748       0.867       0.828       0.893       0.562\n",
      "          pedestrian        2244        1332       0.727       0.543       0.613       0.244\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0.7973857093665015,\n",
       "  0.6855405413755431,\n",
       "  0.7531716917426254,\n",
       "  0.4030898796118148,\n",
       "  0.036653388291597366,\n",
       "  0.04719178006052971,\n",
       "  0.0010762033052742481),\n",
       " array([    0.56187,      0.2443]),\n",
       " 1627586715.8881018,\n",
       " {'p': array([    0.86745,     0.72732]),\n",
       "  'r': array([    0.82842,     0.54266]),\n",
       "  'ap': array([    0.56187,      0.2443]),\n",
       "  'f1': array([    0.84749,     0.62157]),\n",
       "  'ap_class': array([0, 1]),\n",
       "  'ap50': array([    0.89337,     0.61297]),\n",
       "  'mp': 0.7973857093665015,\n",
       "  'mr': 0.6855405413755431,\n",
       "  'map50': 0.7531716917426254,\n",
       "  'map': 0.4030898796118148,\n",
       "  'nt': array([8748, 1332], dtype=int64),\n",
       "  'names': {0: 'car', 1: 'pedestrian'},\n",
       "  'res_per_class': [('car',\n",
       "    2244,\n",
       "    8748,\n",
       "    0.867454642237468,\n",
       "    0.8284179240969365,\n",
       "    0.8933718715658732,\n",
       "    0.5618748728565351),\n",
       "   ('pedestrian',\n",
       "    2244,\n",
       "    1332,\n",
       "    0.727316776495535,\n",
       "    0.5426631586541497,\n",
       "    0.6129715119193777,\n",
       "    0.24430488636709446)],\n",
       "  'conf_matrix': array([[       7668,           3,        2227],\n",
       "         [          1,         856,         600],\n",
       "         [       1079,         470,           0]]),\n",
       "  'losses': [0.036653388291597366,\n",
       "   0.04719178006052971,\n",
       "   0.0010762033052742481]})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate.evaluate('pseudo_fl/fed-12-280.pt', None, None, dataloader, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "helpful-chapel",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  5639932 torch 1.8.0+cu111 CUDA:0 (GeForce RTX 3080, 10240.0MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 224 layers, 7056607 parameters, 0 gradients\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36/36 [00:15<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "saving json runs\\val\\fed-12-200\\fed-12-200_predictions.json...\n",
      "                 all        2244       10080       0.776       0.699       0.749       0.401\n",
      "                 car        2244        8748       0.839       0.845       0.889       0.561\n",
      "          pedestrian        2244        1332       0.713       0.553       0.609       0.242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0.7757478192206549,\n",
       "  0.6993768459817843,\n",
       "  0.7489088930854142,\n",
       "  0.4014253977873569,\n",
       "  0.03673572465777397,\n",
       "  0.04773494601249695,\n",
       "  0.001078975386917591),\n",
       " array([    0.56111,     0.24174]),\n",
       " 1627563495.0057209,\n",
       " {'p': array([    0.83874,     0.71276]),\n",
       "  'r': array([    0.84545,      0.5533]),\n",
       "  'ap': array([    0.56111,     0.24174]),\n",
       "  'f1': array([    0.84208,     0.62299]),\n",
       "  'ap_class': array([0, 1]),\n",
       "  'ap50': array([    0.88882,     0.60899]),\n",
       "  'mp': 0.7757478192206549,\n",
       "  'mr': 0.6993768459817843,\n",
       "  'map50': 0.7489088930854142,\n",
       "  'map': 0.4014253977873569,\n",
       "  'nt': array([8748, 1332], dtype=int64),\n",
       "  'names': {0: 'car', 1: 'pedestrian'},\n",
       "  'res_per_class': [('car',\n",
       "    2244,\n",
       "    8748,\n",
       "    0.8387362298395743,\n",
       "    0.8454503886602652,\n",
       "    0.8888245039975653,\n",
       "    0.5611081534949793),\n",
       "   ('pedestrian',\n",
       "    2244,\n",
       "    1332,\n",
       "    0.7127594086017356,\n",
       "    0.5533033033033034,\n",
       "    0.608993282173263,\n",
       "    0.24174264207973448)],\n",
       "  'conf_matrix': array([[       7763,           2,        2473],\n",
       "         [          4,         850,         604],\n",
       "         [        981,         477,           0]]),\n",
       "  'losses': [0.03673572465777397, 0.04773494601249695, 0.001078975386917591]})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate.evaluate('pseudo_fl/fed-12-200.pt', None, None, dataloader, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alone-timer",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  5639932 torch 1.8.0+cu111 CUDA:0 (GeForce RTX 3080, 10240.0MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 224 layers, 7056607 parameters, 0 gradients\n",
      "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36/36 [00:16<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "saving json runs\\val\\fed-12-121\\fed-12-121_predictions.json...\n",
      "                 all        2244       10080       0.779       0.674       0.745       0.391\n",
      "                 car        2244        8748       0.834       0.825       0.887       0.546\n",
      "          pedestrian        2244        1332       0.725       0.523       0.603       0.236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0.7793613773252648,\n",
       "  0.6736047318466157,\n",
       "  0.7450895779553562,\n",
       "  0.3906007434822122,\n",
       "  0.037519458681344986,\n",
       "  0.04604988545179367,\n",
       "  0.0010285453172400594),\n",
       " array([    0.54552,     0.23568]),\n",
       " 1627513494.777964,\n",
       " {'p': array([      0.834,     0.72473]),\n",
       "  'r': array([    0.82469,     0.52252]),\n",
       "  'ap': array([    0.54552,     0.23568]),\n",
       "  'f1': array([    0.82932,     0.60723]),\n",
       "  'ap_class': array([0, 1]),\n",
       "  'ap50': array([    0.88734,     0.60284]),\n",
       "  'mp': 0.7793613773252648,\n",
       "  'mr': 0.6736047318466157,\n",
       "  'map50': 0.7450895779553562,\n",
       "  'map': 0.3906007434822122,\n",
       "  'nt': array([8748, 1332], dtype=int64),\n",
       "  'names': {0: 'car', 1: 'pedestrian'},\n",
       "  'res_per_class': [('car',\n",
       "    2244,\n",
       "    8748,\n",
       "    0.8339953737837106,\n",
       "    0.824686941170709,\n",
       "    0.8873396108267833,\n",
       "    0.5455226080248707),\n",
       "   ('pedestrian',\n",
       "    2244,\n",
       "    1332,\n",
       "    0.724727380866819,\n",
       "    0.5225225225225225,\n",
       "    0.6028395450839292,\n",
       "    0.23567887893955364)],\n",
       "  'conf_matrix': array([[       7638,           2,        2484],\n",
       "         [          0,         812,         492],\n",
       "         [       1110,         518,           0]]),\n",
       "  'losses': [0.037519458681344986,\n",
       "   0.04604988545179367,\n",
       "   0.0010285453172400594]})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate.evaluate('pseudo_fl/fed-12-200.pt', None, None, dataloader, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff7c847",
   "metadata": {},
   "source": [
    "## Evaluating 2-class sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cedf9c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jelte\\PycharmProjects\\ALFL-clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval2: \u001b[0mScanning 'KITTI\\yolo2\\val.cache' images and labels... 2244 found, 0 missing, 0 empty, 0 corrupted: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2244/2244 [00:00<?, ?it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dataloader = evaluate.get_dataloader(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec08d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob('runs/*/weights/best.pt'):\n",
    "    evaluate.evaluate(file, None, None, dataloader, 2)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407df7d8",
   "metadata": {},
   "source": [
    "## Evaluating 8-class sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cb6024d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imgsz' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\YELESH~1\\AppData\\Local\\Temp/ipykernel_13788/1027888602.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_dataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'KITTI/yolo8/val.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgsz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolorstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'val8: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'imgsz' is not defined"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader(f'KITTI/yolo8/val.txt', imgsz, batch_size, gs, opt, pad=0.5, rect=True, prefix=colorstr('val8: '))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c68df1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Experiments\\ALFL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval8: \u001b[0mScanning \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dataloader = evaluate.get_dataloader(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94287207",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  db3f921 torch 1.10.0 CUDA:0 (Quadro P620, 4096.0MB)\n",
      "\n",
      "Fusing layers... \n",
      "C:\\Users\\YeleshettyD\\Anaconda3\\envs\\AccFL\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Model Summary: 224 layers, 7072789 parameters, 0 gradients, 16.4 GFLOPS\n",
      "               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "saving json runs\\val\\fed-17-200\\fed-17-200_predictions.json...\n",
      "                 all        2244       12341       0.641       0.491       0.503       0.249\n",
      "                 car        2244        8748       0.803       0.837       0.869       0.538\n",
      "             cyclist        2244         496       0.689         0.3       0.347       0.116\n",
      "                misc        2244         293       0.564       0.199       0.257       0.128\n",
      "          pedestrian        2244        1332       0.638       0.462       0.483       0.174\n",
      "      person_sitting        2244          87       0.519       0.172       0.175      0.0494\n",
      "                tram        2244         154       0.622       0.682       0.639       0.283\n",
      "               truck        2244         326        0.68       0.607        0.61       0.346\n",
      "                 van        2244         905       0.612       0.666       0.643       0.359\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0.641021330242898,\n",
       "  0.49072437086155307,\n",
       "  0.5028105517261979,\n",
       "  0.2491455235189361,\n",
       "  0.041197486221790314,\n",
       "  0.06146674603223801,\n",
       "  0.012241465039551258),\n",
       " array([    0.53827,     0.11624,      0.1278,     0.17358,     0.04943,     0.28256,     0.34605,     0.35924]),\n",
       " 1646355181.0216036,\n",
       " {'p': array([    0.80344,     0.68947,     0.56444,     0.63784,     0.51942,     0.62199,     0.67961,     0.61196]),\n",
       "  'r': array([    0.83711,     0.29992,     0.19904,     0.46246,     0.17241,     0.68182,     0.60736,     0.66568]),\n",
       "  'ap': array([    0.53827,     0.11624,      0.1278,     0.17358,     0.04943,     0.28256,     0.34605,     0.35924]),\n",
       "  'f1': array([    0.81993,       0.418,      0.2943,     0.53617,     0.25889,     0.65053,     0.64146,     0.63769]),\n",
       "  'ap_class': array([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       "  'ap50': array([     0.8687,     0.34741,     0.25675,     0.48288,     0.17524,     0.63851,     0.60977,     0.64323]),\n",
       "  'mp': 0.641021330242898,\n",
       "  'mr': 0.49072437086155307,\n",
       "  'map50': 0.5028105517261979,\n",
       "  'map': 0.2491455235189361,\n",
       "  'nt': array([8748,  496,  293, 1332,   87,  154,  326,  905], dtype=int64),\n",
       "  'names': {0: 'car',\n",
       "   1: 'cyclist',\n",
       "   2: 'misc',\n",
       "   3: 'pedestrian',\n",
       "   4: 'person_sitting',\n",
       "   5: 'tram',\n",
       "   6: 'truck',\n",
       "   7: 'van'},\n",
       "  'res_per_class': [('car',\n",
       "    2244,\n",
       "    8748,\n",
       "    0.803440500559343,\n",
       "    0.8371056241426612,\n",
       "    0.8686989732370012,\n",
       "    0.5382740208950806),\n",
       "   ('cyclist',\n",
       "    2244,\n",
       "    496,\n",
       "    0.6894667708190535,\n",
       "    0.2999168523362074,\n",
       "    0.34741309021592187,\n",
       "    0.1162392791160867),\n",
       "   ('misc',\n",
       "    2244,\n",
       "    293,\n",
       "    0.564441858951174,\n",
       "    0.1990368747125504,\n",
       "    0.2567453027297987,\n",
       "    0.1277952771084693),\n",
       "   ('pedestrian',\n",
       "    2244,\n",
       "    1332,\n",
       "    0.6378405739222739,\n",
       "    0.4624624624624625,\n",
       "    0.4828766929321674,\n",
       "    0.1735774843021723),\n",
       "   ('person_sitting',\n",
       "    2244,\n",
       "    87,\n",
       "    0.5194177141016221,\n",
       "    0.1724137931034483,\n",
       "    0.17524198982274652,\n",
       "    0.04943035330469026),\n",
       "   ('tram',\n",
       "    2244,\n",
       "    154,\n",
       "    0.6219924929860826,\n",
       "    0.6818181818181818,\n",
       "    0.638506000093413,\n",
       "    0.2825606191154396),\n",
       "   ('truck',\n",
       "    2244,\n",
       "    326,\n",
       "    0.6796088207965062,\n",
       "    0.6073619631901841,\n",
       "    0.6097696478077285,\n",
       "    0.34604899525507593),\n",
       "   ('van',\n",
       "    2244,\n",
       "    905,\n",
       "    0.6119619098071288,\n",
       "    0.665679215126729,\n",
       "    0.6432327169708065,\n",
       "    0.359238159054474)],\n",
       "  'conf_matrix': array([[       7614,           3,          19,           3,           0,           1,           6,         150,        2931],\n",
       "         [          2,         160,           7,          28,           7,           0,           0,           0,         150],\n",
       "         [          7,          13,          57,           0,          12,           1,          27,          27,         154],\n",
       "         [          1,          51,           2,         711,           9,           0,           0,           0,         601],\n",
       "         [          0,           1,           0,           3,           8,           0,           0,           0,          38],\n",
       "         [          1,           0,           1,           0,           0,          83,          28,           0,         119],\n",
       "         [          4,           0,          21,           0,           0,          41,         146,          13,         219],\n",
       "         [         91,           0,          58,           1,           0,           2,          57,         565,         637],\n",
       "         [       1027,         268,         128,         586,          51,          26,          62,         150,           0]]),\n",
       "  'losses': [0.041197486221790314, 0.06146674603223801, 0.012241465039551258]})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate.evaluate('pseudo_fl/fed-17-200.pt', None, None, dataloader, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf710b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = 8\n",
    "for file in glob.glob(\"D:\\\\Research Project\\\\8_Chained\\\\Runs\\\\*\\\\weights\\\\best.pt\", recursive=True):\n",
    "    evaluate.evaluate(file, None, None, dataloader, 8)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
